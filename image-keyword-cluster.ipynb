{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c638deb5-f7bb-42f9-80ad-c8c458108003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the image directory path\n",
    "image_directory = '/Users/â€¦'\n",
    "\n",
    "# Define number of clusters (i.e. folders or buckets)\n",
    "k = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a885c42-7017-4b46-8578-1213af18cb7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error: File is empty - /Users/benmunson/Downloads/90s Art School Copy/3275231818248093226.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: File is empty - /Users/benmunson/Downloads/90s Art School Copy/3275231818248093226.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error: File is empty - /Users/benmunson/Downloads/90s Art School Copy/3275231818248093226 2.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: File is empty - /Users/benmunson/Downloads/90s Art School Copy/3275231818248093226 2.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benmunson/opt/anaconda3/envs/Overtaker/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/benmunson/opt/anaconda3/envs/Overtaker/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import shutil\n",
    "import re\n",
    "\n",
    "\n",
    "image_data = {}\n",
    "\n",
    "if os.path.isdir(image_directory):\n",
    "    for image_file in os.listdir(image_directory):\n",
    "        if image_file.endswith(('.jpg', '.jpeg', '.png')):\n",
    "            image_path = os.path.join(image_directory, image_file)\n",
    "\n",
    "            # Extract keywords using exiftool\n",
    "            exiftool_command = f'exiftool -XMP:Subject -S -s \"{image_path}\"'\n",
    "\n",
    "            # Execute exiftool command and capture the output\n",
    "            exiftool_process = subprocess.Popen(exiftool_command, stdout=subprocess.PIPE, shell=True)\n",
    "            exiftool_output = exiftool_process.stdout.read().decode('utf-8').strip()\n",
    "\n",
    "            if exiftool_output:\n",
    "                image_data[image_path] = exiftool_output\n",
    "            else:\n",
    "                print(f\"Error: File is empty - {image_path}\")\n",
    "\n",
    "    if image_data:\n",
    "         # Create TF-IDF matrix from keywords\n",
    "        keywords = list(image_data.values())\n",
    "\n",
    "        # Custom tokenizer that treats multi-word tags as single entities without splitting them further\n",
    "        def custom_tokenizer(text):\n",
    "            split_by_semicolon = [phrase.strip() for phrase in text.split(';')]\n",
    "            return split_by_semicolon\n",
    "\n",
    "        # Create a custom TfidfVectorizer with the custom tokenizer\n",
    "        vectorizer = TfidfVectorizer(tokenizer=custom_tokenizer)\n",
    "        tfidf_matrix = vectorizer.fit_transform(keywords)\n",
    "\n",
    "        # Apply k-means clustering\n",
    "        kmeans = KMeans(n_clusters=k)\n",
    "        kmeans.fit(tfidf_matrix)\n",
    "\n",
    "        # Create directories for each cluster\n",
    "        for cluster_num in range(k):\n",
    "            cluster_dir = os.path.join(image_directory, f'Cluster_{cluster_num}')\n",
    "            os.makedirs(cluster_dir, exist_ok=True)\n",
    "\n",
    "       # Copy images to their respective cluster folders\n",
    "        for i, (image_path, keyword) in enumerate(image_data.items()):\n",
    "            cluster = kmeans.labels_[i]\n",
    "            cluster_dir = os.path.join(image_directory, f'Cluster_{cluster}')\n",
    "            image_name = os.path.basename(image_path)\n",
    "            new_image_path = os.path.join(cluster_dir, image_name)\n",
    "            shutil.copy(image_path, new_image_path)\n",
    "        \n",
    "    else:\n",
    "        print(\"No valid images with keywords found.\")\n",
    "else:\n",
    "    print(f\"Error: Directory {image_directory} not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f2286da-0e53-4b41-b62a-78841a87852a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Centroids:\n",
      "Top keywords information saved to /Users/benmunson/Downloads/90s Art School Copy/cluster_top_keywords.txt in the main folder.\n"
     ]
    }
   ],
   "source": [
    "# Get cluster centroids\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "# Print cluster centroids\n",
    "print(\"Cluster Centroids:\")\n",
    "for i, centroid in enumerate(centroids):\n",
    "\n",
    "    # Find the top keywords for each cluster based on the centroid\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    num_top_keywords = 10  # Number of top keywords to display\n",
    "\n",
    "for i, centroid in enumerate(centroids):\n",
    "    top_keyword_indices = centroid.argsort()[-num_top_keywords:][::-1]\n",
    "    top_keywords = [feature_names[ind] for ind in top_keyword_indices]\n",
    "    \n",
    "# Define the path for the output text file within the image_directory\n",
    "output_file_path = os.path.join(image_directory, \"cluster_top_keywords.txt\")\n",
    "\n",
    "# Write the top keywords information to the text file\n",
    "with open(output_file_path, 'w') as file:\n",
    "    for i, centroid in enumerate(centroids):\n",
    "        top_keyword_indices = centroid.argsort()[-num_top_keywords:][::-1]  # Select the top three keywords\n",
    "        top_keywords = [feature_names[ind] for ind in top_keyword_indices]\n",
    "        \n",
    "        file.write(f\"Top keywords for Cluster {i + 1}:\\n\")\n",
    "        for j, keyword in enumerate(top_keywords):\n",
    "            file.write(f\"{j+1}. {keyword}\\n\")\n",
    "        file.write(\"\\n\")\n",
    "\n",
    "print(f\"Top keywords information saved to {output_file_path} in the main folder.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
